{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1 Data analysis with sinusoid of unknown amplitude in known Gaussian noise</h1>\n",
    "\n",
    "<p class=lead>\n",
    "    The problem you are faced with is detecting and characterizing a sinusoid\n",
    "    in Gaussian noise.  This is a standard \n",
    "    and simple problem in data analysis that will give you a flavor of the data\n",
    "    analysis problems and techniques used in real PTA data analysis. In fact, \n",
    "    nearly the entire GW detection problem is framed in this way: Is there a\n",
    "    detectable GW (be it stochastic, continuous, or burst) present in the puslar\n",
    "    timing residual data? The challenge is coming up with robust and efficient \n",
    "    analysis methods that take into account all of the intracacies of real puslar\n",
    "    timing data.\n",
    "    <br/><br/>\n",
    "    Dealing with real or simulated PTA data will only cause additional complications\n",
    "    and will deter you from learning the basics of data analysis. In this activity \n",
    "    you will explore both frequentist and Bayesian techniques for parameter estimation, \n",
    "    detection, and setting upper limits. <br/><br/>\n",
    "    We assume that the data $d$ is composed of a signal $s$ plus additive white gaussian \n",
    "    noise $n$, that is\n",
    "    $$\n",
    "    d=s+n.\n",
    "    $$\n",
    "    Since $n$ is assumed to be white and Gaussian, its probability distribution is a product of\n",
    "    Gaussians\n",
    "    $$\n",
    "    p(n)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\prod_i\\exp\\left(-\\frac{n_i^2}{2\\sigma^2}\\right),\n",
    "    $$\n",
    "    where $n_i$ is the noise at time $t_i$ and $\\sigma$ is the standard deviation of the noise. \n",
    "    We can now write this in terms of the data using $n=d-s$, therefore the likelihood function\n",
    "    for the data given the signal parameters, $\\lambda$ is\n",
    "    $$\n",
    "    p(d|\\lambda)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\prod_i\\exp\\left(-\\frac{(d_i-s_i(\\lambda))^2}{2\\sigma^2}\\right).\n",
    "    $$\n",
    "    For our purposes we will deal with the natural log of the likelihood ratio. The likelihood ratio is the \n",
    "    likelihood function above divided by the likelihood that the signal is pure noise (i.e, remove\n",
    "    the signal $s$ from the model). We will also introduce a new notation and write the log-likelihood \n",
    "    ratio\n",
    "    $$\n",
    "    \\log\\Lambda=(d|s)-\\frac{1}{2}(s|s),\n",
    "    $$\n",
    "    where the inner product of two timeseries $x$ and $y$, for example, is $(x|y)=\\sum_i x_iy_i/\\sigma^2$.\n",
    "    In our case the signal $s$ is just a sinusoid, $s(t,A,f) = A\\sin(2\\pi ft)$. For simplicity we will \n",
    "    assume that we know the frequency (or that we are doing a \"target\" search for a certain frequency) \n",
    "    of the sinusoid and the only unknown parameter is the amplitude.\n",
    "    <br/><br/>\n",
    "    Your first exercise will be to create a simulated dataset with white noise and a small sinusoid. \n",
    "    <br/><br/>\n",
    "    Good luck!\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import worksheet_utils as wu\n",
    "from scipy.interpolate import interp1d\n",
    "import simple_mcmc as smcmc\n",
    "from corner import corner\n",
    "import nestle\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.0 Derving the log-likelihod ratio</h2>\n",
    "\n",
    "<p class='lead'>\n",
    "    Above we have written down the log of the likehood ratio which will be used throughout this \n",
    "    entire worksheet. To get yourself familiar with the notation, derive the log-likelihood ratio\n",
    "    defined above. \n",
    "    <br/><br/>\n",
    "    <em>Hint</em>: It will be easier to take the logarithm of each likelihood function first and\n",
    "    then take the difference (i.e., $\\log(A/B)-\\log(A)-\\log(B)$).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.1 Simulating the data set. </h2>\n",
    "\n",
    "<p class=lead>\n",
    "    Create a 5 year timeseries of 130 points using `numpy` function <strong>`linspace()`</strong> beginning at 0. \n",
    "    Then, use the <strong>`simData()`</strong> function from `worksheet_utils` to simulate some fake data with a standard devitaion of \n",
    "    100 ns and a signal with amplitude 70 ns and frequency of $10^{-8}$ Hz.\n",
    "    <br/><br/>\n",
    "    Then use the `matplotlib` function __`plot()`__ to plot the data vs. time and be sure to label the axes. Next, create a separate dataset with no noise by calling the __`signal()`__ function from `worksheet_utils` in order to plot just the signal on top of the signal+noise dataset.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define frequency, Amplitude, timeseries and sigma\n",
    "\n",
    "# call function to simulate data\n",
    "\n",
    "# call wu.signal function to simulate sine wave individually\n",
    "\n",
    "# plot data in blue with signal in red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.2 Parameter Estimation: Estimating the Amplitude</h2>\n",
    "\n",
    "<p class=lead>\n",
    "    In frequentist statistics, one is interested in defining some estimator for the true signal parameters.\n",
    "    A common way to do this is to find the parameters that maximize the likelihood. These are known as Maximum\n",
    "    Likelihood Estimators (MLEs). One can then place confidence intervals on the parameter of interest through the \n",
    "    <em>Neyman method</em>: define some statistic $x$ that is a function of the data $d$ (for example the MLE of the\n",
    "    unknown signal parameters). Because $x$ is derived from $d$, the probability distribution (likelihood) $p(d|\\lambda)$\n",
    "    can be re-expressed as a probability distribution $p(x|\\lambda)$. Then, for each value of $\\lambda$, we produce an \n",
    "    interval $(x_1,x_2)$ such that\n",
    "    $$\n",
    "    \\alpha = \\int_{x_1}^{x_2}dx\\,\\, p(x|\\lambda)\n",
    "    $$\n",
    "    The intervals that depend on $\\lambda$ define a belt in the $\\lambda-x$ plane and for any <em>observed</em> value\n",
    "    of $x$ in the experiment, the confidence interval on $\\lambda$ constitutes the values of $\\lambda$ that exist on the \n",
    "    belt at that fixed value of $x$. (See additional handout for more details)\n",
    "    <br/><br/>\n",
    "    In Bayesian statistics, one is interested finding the probability distribution function for the unknown parameters given\n",
    "    that we have some observed data. To get a point estimate of the unkwnown parameters we may use the <em>maximum a-posteriori</em>\n",
    "    (MAP) parameters, that is, the maximum of the posterior probability distribution. A credible interval (Bayesian version\n",
    "    of confidence intervals) can be constructed directly from the posterior distribution $p(\\lambda|d)$ as\n",
    "    $$\n",
    "    \\alpha = \\int_{\\lambda_1}^{\\lambda_2}d\\lambda\\,\\, p(\\lambda|d),\n",
    "    $$\n",
    "    where $\\lambda_1$ and $\\lambda_2$ defined the lower and upper bounds on our parameter $\\lambda$.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2.1 MLE for the amplitude</h3>\n",
    "\n",
    "<p class=lead>\n",
    "    As written above, the log-likelihood ratio is \n",
    "    $$\n",
    "    \\log\\Lambda=(d|s)-\\frac{1}{2}(s|s).\n",
    "    $$\n",
    "    If we let $s(t)=A\\sin(2\\pi f t)$ and $\\tilde{s}(t)=\\sin(2\\pi f t)$, then $s(t)=A\\tilde{s}(t)$. \n",
    "    Re-writing the log-likehood ratio, we obtain\n",
    "    $$\n",
    "    \\log\\Lambda=A(d|\\tilde{s})-\\frac{A^2}{2}(\\tilde{s}|\\tilde{s}).\n",
    "    $$\n",
    "    We can now maximize the likehood function over the unknown amplitude, $A$, as follows\n",
    "    $$\n",
    "    0 = \\frac{\\partial \\log\\Lambda}{\\partial A} = (d|\\tilde{s}) -\\hat{A}(\\tilde{s}|\\tilde{s}),\n",
    "    $$\n",
    "    where $\\hat{A}$ is the MLE for $A$. Solving for $\\hat{A}$, we obtain\n",
    "    $$\n",
    "    \\hat{A}=\\frac{(d|\\tilde{s})}{(\\tilde{s}|\\tilde{s})}.\n",
    "    $$\n",
    "    The variance on the maximum likelihood estimator is then\n",
    "    $$\n",
    "    \\sigma_{\\hat{A}}^2 = \\langle \\hat{A}\\hat{A} \\rangle -\\langle \\hat{A} \\rangle^2 = \\frac{1}{(\\tilde{s}|\\tilde{s})},\n",
    "    $$\n",
    "    where $\\langle  \\rangle$ denotes the <em>expectation value</em> or average over many realizations of data.\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        Write a function that reads in the data, the time samples, the frequency of the sine wave and the \n",
    "        standard deviation of the noise as arguments and outputs the MLE and standard deviation of $A$. \n",
    "        <em>Hint:</em> Look at the `worksheet_utils` package to figure out what functions you may need.<br/>\n",
    "        <span style=\"color:red\">For intermediate/expert level: Derive the variance of $\\hat{A}$. <em>Hint:</em> \n",
    "        $\\langle (n|\\tilde{s}) \\rangle=0$ and $\\langle (n|\\tilde{s})(n|\\tilde{s}) \\rangle=(\\tilde{s}|\\tilde{s})$.</span>\n",
    "    </li>\n",
    "    <li>\n",
    "        Use this function to compute $\\hat{A}$ for our simulated data set.\n",
    "    </li>   \n",
    "    <li>Does the answer you get make sense? What is the percent error?</li>\n",
    "</ol>\n",
    "</p>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  maximum likelihood estimator for A. fill in function\n",
    "def maxLikeA(data, t, f, sigma):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call maximum likelihood function and print MLE along with the true value of A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2.2 Posterior distribution for the amplitude</h3>\n",
    "\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        Write a function that reads in the data, the time samples, the amplitude of the sine wave, the frequency of the sine wave\n",
    "        and the standard deviation of the noise and returns the log-likelihood ratio.\n",
    "    </li>\n",
    "    <li>\n",
    "        Create a vector of trial amplitudes in the range $[0, 5\\times 10^{-7}]$ with length 1000 using\n",
    "        the __`np.linspace()`__ function. You can play around with the length of the array and the ranges\n",
    "        once you have done this once.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Loop over these amplitudes and compute the log-posterior for each one and store\n",
    "        it in an array. Here we will use a uniform prior on $A$ which means that the likelihood\n",
    "        and the posterior are identical except for a normalizing constant which is not important \n",
    "        here. The posterior is then\n",
    "        $$\n",
    "        \\log\\,p(A|d) \\propto \\log\\Lambda(d|A) + \\log p(A),\n",
    "        $$\n",
    "        where p(A) is the prior. \n",
    "    </li>\n",
    "    <li>\n",
    "        This prior must be normalized to 1, so if our prior is uniform in $A$ then\n",
    "        it is the save for every value of $A$, thus $p(A)=C$, where $C$ is a constant. \n",
    "        Integrate this uniform prior to determine\n",
    "        the normalization constant (i.e., use $\\int p(A)dA=1$ to find the value of $C$).\n",
    "    </li>   \n",
    "    <li>\n",
    "        Use the __`plt.plot()`__ function to plot the posterior vs. the \n",
    "        Amplitude Array.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Determine the MAP value of $A$ by using the __`np.argmax()`__ function to find the array index\n",
    "        of the maximum posterior value. You can then use this array index to find the corresponding\n",
    "        value of $A$ from the vector of trial amplitudes that you created in step 2.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Determine the standard deviation of $A$. <em>Hint:</em> recall that the statistical definition\n",
    "        of variance on a parameter $x$ whose pdf is $p(x)$ is $\\sigma^2_x=\\langle x^2 \\rangle - \\langle x \\rangle^2 \n",
    "        = \\int dx \\, x^2\\,p(x) - \\left(\\int dx\\, x\\, p(x)\\right)^2$\n",
    "    </li>    \n",
    "    <li>\n",
    "        Plot the likelihood vs $A$. Plot a vertical line denoting the true value of $A$ with the __`plt.axvline()`__ \n",
    "        function. Also, use the __`plt.axvspan()`__ with the <em>alpha</em> argument to plot a shaded region denoting\n",
    "        the standard deviation of $A$.\n",
    "    </li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# log-likelihood ratio function (fill in function)\n",
    "def LogLikelihood(data, t, A, f, sigma):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create amplitude vector and initialize log likelihood vector\n",
    "\n",
    "# loop over amplitude value and call log likelihood function for each amplitude\n",
    "\n",
    "# exponentiate log-likelihood and multiply by prior to get the posterior distribution\n",
    "\n",
    "# use argmax function to find index of maximum value of the posterior. Use array index to find MAP value of A\n",
    "\n",
    "# print MAP value of A and true value\n",
    "\n",
    "# plot posterior vs A in blue and vertical line at the injected value in red\n",
    "\n",
    "# use axhspan to make shaded region denoting 1 standard deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.2.2.1: Gaussian prior distribution on $A$</h4>\n",
    "\n",
    "<p class=lead>\n",
    "    It is often the case in data analysis where we have some additional prior information about our problem before\n",
    "    conducting the experiment. In the Bayesian framework we incorporate this additional information into the prior\n",
    "    probability distribution. The data (via the likelihood function) is then used to update our prior knowledge. If\n",
    "    our data is <em>informative</em> then the posterior distribution will be different then the prior distribution. \n",
    "    On the other hand, if our posterior results in distribution that is identical to the prior then our data is not\n",
    "    informative.\n",
    "    <br/><br/>\n",
    "    To illustrate this, we will assume that through some other experiment, or from some theoretical prediction, we have\n",
    "    some prior knowledge of the amplitude of the sine wave in our data. We will model this as a gaussian prior with the\n",
    "    mean the true value that we used to create the data and some standard deviation\n",
    "    $$\n",
    "    p(A)=\\frac{1}{\\sqrt{2\\pi\\sigma_A^2}}\\exp\\left(-\\frac{(A-A_{\\rm true})^2}{2\\sigma_A^2}\\right)\n",
    "    $$\n",
    "</p>\n",
    "\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        Let the fractional uncertainty on $A$ be 25%, that is $\\sigma_A=0.25\\times A$. Use this information in the\n",
    "        gaussian prior and repeat the steps from exercise 2.2. You will need to create an extra array to store the \n",
    "        prior function when conducting the for loop. When constructing the posterior, remember to  multiply the \n",
    "        likelihood by the prior.\n",
    "    </li>\n",
    "    <li>\n",
    "        Make sure that the posterior and prior both are both normalized and plot the posterior and the prior vs. A.\n",
    "        Is the data informative?\n",
    "    </li>\n",
    "    <li>\n",
    "        Repeat this exercise for $\\sigma_A=0.05\\times A$.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# repeat above exercise with the gaussian prior shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2.3 Frequentist Confidence Interval on Amplitude Parameter</h3>\n",
    "\n",
    "<p class=lead>\n",
    "    As we show above, contructing a frequentist confidence interval involves simulating several realizations of data with different values of the sinusoid amplitude. Here we will construct a function to do just that.\n",
    "</p>\n",
    "\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        Create a vector of trial amplitudes in the range $[1\\times 10^{-8}, 3\\times 10^{-7}]$ with length 100 using\n",
    "        the __`linspace()`__ function.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Loop over these amplitudes as above but now add a second inner loop to simulate different <em>\n",
    "        realizations</em> of the data. Use __`simData()`__ function to simulate the new data. For each\n",
    "        amplitude do 1000 realizations of data with that amplitude. (See code block below for hints)\n",
    "    </li>    \n",
    "    <li>\n",
    "        The goal here is to compute the MLE for $A$ for each realization of data. For a given amplitude,\n",
    "        compute $\\hat{A}$ for each data realization using your function from exercise 2.1 and store it in an\n",
    "        array. Then use the __`confinterval()`__ function from `worksheet_utils` to define the upper and lower 1-sigma bounds on the\n",
    "        distribution of $\\hat{A}$ for a given value of $A$. Store these 1-sigma lower and upper\n",
    "        bounds in arrays. This will create the band through the $A$-$\\hat{A}$ space as we saw in the\n",
    "        talk. (See code block below for hints)\n",
    "    </li>    \n",
    "    <li>\n",
    "        After you have looped over all values of the amplitude. Use the <a href=\"http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html\">__`interp1d()`__</a> function to\n",
    "        interpolate the 1-sigma upper and lower bounds on $\\hat{A}$ for each value of $A$.\n",
    "        This way we create an interpolating function so that we can evaluate the\n",
    "        confidence interval on $A$ from our confidence belt and our measurement of $\\hat{A}$. \n",
    "    </li> \n",
    "    <li>\n",
    "        Plot the confidence band (i.e., the injected amplitude vs the 1-sigma upper and lower bounds on $\\hat{A}$.) as\n",
    "        well as a vertical line denoting the measured value of $\\hat{A}$ and a shaded area using the __`plt.axhspan()`__\n",
    "        function to denote the confidence interval on $A$.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set number of amplitudes and create amplitude vector\n",
    "\n",
    "# set number of realizations and intialize arrays for the 1-sigma bounds on Ahat\n",
    "\n",
    "# loop over amplitudes\n",
    "\n",
    "# for ii in range(N_amplitudes):\n",
    "\n",
    "    # loop over realizations\n",
    "    # for jj in range(N_realizations):\n",
    "    \n",
    "        # simulate data set and get Ahat for new dataset\n",
    "        \n",
    "    # get 1-sigma confidence intervals on Ahat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# interpolate to get inverse function for the injected amplitude\n",
    "# as a function of the confidence regions on Ahat. We want two\n",
    "# interpolating functions, one for the upper bounds on Ahat and\n",
    "# one for the lower bounds on Ahat. The x values of the interpolating\n",
    "# function should be the lower/upper bounds on Ahat for each value of A,\n",
    "# and the y values should be the values of A themselves\n",
    "\n",
    "\n",
    "# use the interpolation functions and evaluate them at the measured value of Ahat for our original dataset\n",
    "\n",
    "# print the corresponding confidence region on A\n",
    "\n",
    "# plot the confidence belt, measured value of Ahat and a shaded confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2.4 Bayesian Credible Interval on Amplitude Parameter</h3>\n",
    "\n",
    "<p class=lead>\n",
    "    As was discussed in the talk and have shown above, Bayesian credible intervals use\n",
    "    the likelihood function directly to measure our confidence, or degree of belief\n",
    "    in our measurement of the Amplitude parameter. In practice, this is much less \n",
    "    complicated than constructing frequentist confidence intervals. However in\n",
    "    more complex data analysis problems with many unknown parameters, just constructing\n",
    "    the full likelihood function is very difficult or at least computatinally demanding.\n",
    "</p>\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        Repeat steps 1-3 from exercise 2.2 to construct the likelihood function for $A$.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Write down an algorithm for computing the Bayesian credible region. You do not have\n",
    "        to write actual code here unless you feel comfortable. Simply think about how you \n",
    "        would set up the problem. <em>Hint:</em> Remember, the desired integral is        \n",
    "        $$\n",
    "        \\alpha = \\int_{\\lambda_1}^{\\lambda_2}d\\lambda\\,\\, p(\\lambda|d)\n",
    "        $$\n",
    "        but it can be re-written as        \n",
    "        $$\n",
    "        \\frac{1-\\alpha}{2} = \\int_{-\\infty}^{\\lambda_1}d\\lambda\\,\\, p(\\lambda|d)\n",
    "        $$        \n",
    "        and        \n",
    "        $$\n",
    "        \\frac{1+\\alpha}{2} = \\int^{\\infty}_{\\lambda_2}d\\lambda\\,\\, p(\\lambda|d)\n",
    "        $$\n",
    "    </li>    \n",
    "    <li>\n",
    "        Use the pre-made function __`confinterval_like()`__ to construct the 1-sigma upper\n",
    "        and lower bounds on $A$.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Plot the likelihood function vs. $A$ along with the 1-sigma upper and lower bounds, \n",
    "        again using the __`plt.plot()`__ and __`plt.axvline()`__ functions.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# repeat steps 1-3 of exercise 2.2\n",
    "\n",
    "# exponentiate likelihood function and compute confidence interval\n",
    "\n",
    "# print upper and lower bounds of credible region\n",
    "\n",
    "# plot the likelihood vs. A in blue along with vertical lines for the upper/lower bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.3. Hypothesis Testing: Detecting the Signal</h2>\n",
    "\n",
    "<p class=lead>\n",
    "    Above, when performing parameter estimation we have taken for granted that there was an <em>detectable</em>\n",
    "    signal in the data. Now we turn to the problem of detection, that is, how can we confidently say that there\n",
    "    truly is a signal in the data. In the frequentist framework, detection significance is usually based on the\n",
    "    false alarm probability (sometimes referred to as FAP, I know hilarious right?), that is, the probability \n",
    "    that we would (falsely) claim a detection when that data consists of only noise. Henceforth, we refer to this\n",
    "    noise only case as the <em>null hypothesis</em> and we denote it symbolically as $H_0$. On the other hand we\n",
    "    refer to the case where the data does contain a signal as the <em>signal hypythesis</em> and we denote it symbolically\n",
    "    as $H_1$. In the Bayesian framework, we actually compute the <em>evidence</em> for the signal and null hypotheses and\n",
    "    compare them via the Bayes factor. \n",
    "</p>\n",
    "\n",
    "<p class=lead>\n",
    "    In this next section, we will apply these techniques to our simple problem of the sinusoid with unknown amplitude.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.3.1 Frequentist Hypothesis Testing</h3>\n",
    "\n",
    "<p class=lead>\n",
    "    The false alarm probability is defined as   \n",
    "    $$\n",
    "    {\\rm FAP} = \\int_{\\hat x}^{\\infty} dx\\,\\, p(x|\\lambda,H_0),\n",
    "    $$    \n",
    "    where $\\hat{x}$ is our measured test statistic, $\\lambda$ is the \n",
    "    parameter of interest, and $p(x|\\lambda,H_0)$ is the pdf of $x$ under the null \n",
    "    hypothesis. The dependence on the actual data, comes in the value of $\\hat{x}$. Essentially what we are doing\n",
    "    is we are testing whether our point extimate of $x$ is consistent with a value of $x$ that we would measure if\n",
    "    the data contains only noise. So to claim a detection, we want to reject the null hypothesis, that is, we want \n",
    "    $\\hat{x}$ to be <em>inconsistent</em> with the distribution of $x$ under the null hypothesis and the false alarm\n",
    "    probability quantifies that inconsistency.\n",
    "</p>\n",
    "\n",
    "<p class=lead>\n",
    "    The value of the false alarm probability that one requires for detection is completely problem dependent. In some\n",
    "    cases we would be ok with a value of 5% (i.e., we make a false detection 5% of the time). For example, many of the\n",
    "    social sciences use this value. However, for our purposes we want to be much more confident so we will require \n",
    "    $FAP < 10^{-4}$ for a detection, that is, there is a 1 in 10,000 chance that we have made a false detection. It is\n",
    "    very important to note here that just because we can rule out the null hypothesis with $1-FAP$ confidence, does \n",
    "    <em>not</em> mean that the signal hypothesis is true with $1-FAP$ confidence. In fact, frequentist detection methods \n",
    "    make no statetments about our confidence in the signal model itself, only that a measurement made under the signal \n",
    "    hypothesis is inconsistent with the null hypothesis at the $1-FAP$ level.\n",
    "</p>\n",
    "\n",
    "<p class=lead>\n",
    "    Now we want to set the value of $\\hat{x}$ that would be required for us to have $FAP=10^{-4}$. \n",
    "</p>\n",
    "\n",
    "<ol class=lead>    \n",
    "    <li>\n",
    "        First, we want to construct our detection statistic. The <a href=\"http://en.wikipedia.org/wiki/Neymanâ€“Pearson_lemma\">\n",
    "        Neyman-Pearson lemma</a> states that the optimal detection statistic is the likelihood ratio. Use our expression for\n",
    "        the log-likelihood ratio and $\\hat{A}$ from exercise 2.1 to construct the maximum likelihood ratio. <em>Hint:</em>\n",
    "        you should get        \n",
    "        $$\n",
    "        \\log \\Lambda_{\\rm max} = \\frac{1}{2}\\frac{(d|\\tilde s)^2}{(\\tilde s|\\tilde s)}\n",
    "        $$\n",
    "    </li>    \n",
    "    <li>\n",
    "       As we did for the likelihood ratio. Write a function that reads in the data, the time samples, the frequency \n",
    "       of the sine wave and the standard deviation of the noise and returns the maximized log-likelihood ratio.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Now we want to construct the pdf of the maximum log-likelihood ratio under the null hypothesis. As we did\n",
    "        in exercise 2.3, we want to loop over many realizations of data and compute the max log-likelihood ratio,\n",
    "        only this time we simulate only noise. Simulate 100000 realizations of data and store the maximum log-likelihood\n",
    "        values in an array and histogram the results with 50 bins using the __`plt.hist()`__ function.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Now that we have the distribution $p(x|\\lambda,H_0)$, we want to determine the value of $\\hat{x}=\\hat{x}_{\\rm thresh}$\n",
    "        that will give a false alarm probability of $10^{-4}$. We have already build all of the machinery we require to determine\n",
    "        $\\hat{x}_{\\rm thresh}$. <em>Hint:</em> If we re-write the FAP integral, we can use the __`confinterval()`__ function\n",
    "        with the argument onesided=True. Please ask for help if you are stuck here.\n",
    "        <br/>\n",
    "        <span style='color:red'>Expert level: This distribution can be computed analytically. Do you know what it is?</span>\n",
    "    </li>    \n",
    "    <li>\n",
    "        Use our original data with a signal to determine if we have made a detection.\n",
    "    </li>    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# maximum log likelihood function. Fill in function\n",
    "def maxLogLikelihood(data, t, f, sigma):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of realizations and initialize max log-likelihood array\n",
    "\n",
    "# loop over realizations\n",
    "        \n",
    "    # simulate data and calculate maximum log-likelihood value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot histogram of maximum log-likelihood values, use log=True to more clearly see the tail\n",
    "\n",
    "# compute threshold value on the maximum log-likelihood ratio\n",
    "\n",
    "# compute maximum log-likelihood ratio and determine if we have made a detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.3.2 Bayesian Hypothesis Testing</h3>\n",
    "\n",
    "<p class=lead>\n",
    "    When condisering Bayesian parameter estimation, we used Bayes theorem for a <em>single</em> hypothesis. We now use the \n",
    "    more general form of Bayes theorem:    \n",
    "    $$\n",
    "    p(\\lambda,H|d) = \\frac{p(d|\\lambda,H)p(\\lambda,H)}{p(d)},\n",
    "    $$\n",
    "    where $p(\\lambda,H|d)$ is the probability of the joint posterior of parameters and the hypotheses, $p(d|\\lambda,H)$ is the likelihood of the data given a set of parameters and hypotheses, $p(\\lambda,H)=p(\\lambda|H)p(H)$ is the prior probability on the parameters and hypotheses and $p(d)$ is the probability of the data itself, also known as the evidence. Here we are interested in the evidence:    \n",
    "    $$\n",
    "    p(d)=\\sum_i\\int d\\lambda_i\\,p(d|\\lambda_i,H_i)p(\\lambda_i|H_i)p(H_i)= \\sum_i p(d|H_i)p(H_i)\n",
    "    $$\n",
    "    which is the sum of the marginal likelihoods for different Hypotheses. In some cases, there may be many possible hypotheses, but it is\n",
    "    always possible to compare different ones. In our case we want to <em>directly</em> calculate the evidence for hypotheses, $H_1$ and $H_0$.\n",
    "    The bayes factor is defined as the ratio of two marginal likelihoods    \n",
    "    $$\n",
    "    B_{10} = \\frac{p(d|H_1)}{p(d|H_0)}.\n",
    "    $$    \n",
    "    In large parameter spaces, this is notoriously hard to calculate; however, in our case it is trivial since our null \n",
    "    hypothesis model has no parameters and our signal model only has 1 parameter we have    \n",
    "    $$\n",
    "    B_{10}=\\int dA\\,\\, \\Lambda(A|d)p(A)\n",
    "    $$    \n",
    "    where $\\Lambda(A|d)$ is just the likelihood ratio defined above and $p(A)$ is the prior on $A$, which we take to be constant. \n",
    "    For our calculation, we will need to include the prior even though it is a constant. You can analytically find the normalizing\n",
    "    constant by integrating $p(A)=C$ over the range of $A$ (which is $[0,5\\times10^{-7}]$ if using the same as exercise 2.2) and \n",
    "    setting it equal to 1. That is, we require    \n",
    "    $$\n",
    "    \\int dA\\,\\, p(A) = 1\n",
    "    $$\n",
    "</p>\n",
    "\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        Loop over 1000 amplitude values and compute the log likelihood just like in exercise 2.2. Once you have the log-likelihood ratio \n",
    "        at each amplitude value you can exponentiate it to obtain the likelihood itself.\n",
    "    </li>    \n",
    "    <li>\n",
    "        Use the likelihood values to compute the bayes factor. Remember we can just use a block integral        \n",
    "        $$\n",
    "        \\int dA\\,\\, \\Lambda(A|d) \\approx \\sum_i\\Lambda(A_i|d) \\Delta A.\n",
    "        $$        \n",
    "        The value of the Bayes factor that is normally considered decisive evidence for model $H_1$ over model $H_0$ is 100. This \n",
    "        values comes from the so-called <a href=\"http://en.wikipedia.org/wiki/Bayes_factor#Interpretation\">Jeffreys' scale</a>. For\n",
    "        our purposes here we will follow this scale but in real data analysis applications is is an open problem as to which value \n",
    "        one would truly consider decisive, say for claiming the detection of GWs. One could implement a hybrid frequentist-Bayesian\n",
    "        method where the Bayes factor is used as our test statistic and we can set corresponding FAPs and thresholds for detection.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# repeat exercise 1.2.2 steps 1-3\n",
    "\n",
    "\n",
    "# compute bayes factor with blick integral being sure to normalize prior\n",
    "\n",
    "# print Bayes factor. Is it bigger than 100?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.3.2.1 Occam Penalty</h4>\n",
    "\n",
    "<p class=lead>\n",
    "    In hypothesis testing we generally want to choose the simplest model if it provides a good fit to the data. In essence, \n",
    "    we need to weigh the \"goodnes of fit\" versus the simplicity of the model. For example, we could use a free parameter for \n",
    "    every data point, then our model would fit the data perfectly but this model would have hundreds of parameters, versus just\n",
    "    one for our sinusoid model. The bayes factor automatically incorporates this parsimony through the integration of the posterior,\n",
    "    which contains our prior distribution. \n",
    "</p>\n",
    "\n",
    "<ol class=lead>\n",
    "    <li>\n",
    "        We can see this automatic parsimony by choosing a larger prior range on $A$ in the above calculation of the Bayes factor. \n",
    "        Repeat the calculation but now create the amplitude vector going up to $5\\times 10^{-6}$. <em>Hint:</em> Remember to \n",
    "        recalculate the normalization of the prior.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# repeat the above exercise with different bounds on the Amplitude vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data analysis of unkown sinusoid in unknown white noise\n",
    "\n",
    "<p class='lead'>\n",
    "    Here you will perform a similar Bayesian Analysis to the one above but we will be expanding the\n",
    "    number of free parameters from a single unknown amplitude to an unknown, amplitude, frequency, \n",
    "    and phase of the sinusoid plus an unknown white noise level. This will allow us to introduce\n",
    "    multi-parameter likelihoods and Markov Chain Monte-Carlo. Real GW analyses usually have tens to\n",
    "    hundreds of free parameters; however, for this exercise 4 free parameters will give you a feel for\n",
    "    how to run more complex models.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, x, t, pmin=[-9, -9, 0, -9], \n",
    "                 pmax=[-6, -7, 2*np.pi, -6]):\n",
    "        \"\"\"\n",
    "        Model class contains calls to log-likelihood and\n",
    "        log-prior functions. All calls to log-likelihood\n",
    "        and log-prior functions take in parameter array\n",
    "        in the order:\n",
    "        \n",
    "        0: log10 Amplitude [s]\n",
    "        1: log10 frequency [Hz]\n",
    "        2: phase [radian]\n",
    "        3: log10 sigma [s]\n",
    "        \n",
    "        :param x: Data vector\n",
    "        :param t: Times at which data is taken\n",
    "        :param pmin: Minimum values of model parameters\n",
    "        :param pmax: Maximum values of model parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        self.t = t  \n",
    "        self.pmin = np.array(pmin)\n",
    "        self.pmax = np.array(pmax)\n",
    "    \n",
    "    def get_loglike(self, pars):\n",
    "        \n",
    "        # construct signal from parameter vector pars\n",
    "        \n",
    "        # construct the gaussian log-likelihood including the\n",
    "        # log determinant piece\n",
    "    \n",
    "        return loglike\n",
    "    \n",
    "    def get_logprior(self, pars):\n",
    "        \n",
    "        # check to see if all parameters are > pmin and < pmax\n",
    "        # and return prior (what should the prior value be for uniform priors?)\n",
    "        # return -np.inf otherwise\n",
    "\n",
    "        return prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Set up data and model\n",
    "\n",
    "<p class='lead'>\n",
    "    Set up a data set exactly as above in section 1.1 but with phase = 1.5.\n",
    "    You can do this by using the ```phi``` keyword argument to ```wu.simData```\n",
    "    and ```wu.signal```.\n",
    "    \n",
    "    As you go through the next steps in sampling you can simulated different data\n",
    "    sets with different parameters just to get a feel for things.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define frequency, Amplitude, phase, timeseries and sigma\n",
    "\n",
    "# call function to simulate data\n",
    "\n",
    "# call wu.signal function to simulate sine wave individually\n",
    "\n",
    "# plot data in blue with signal in red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup model object by calling Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 MCMC Sampling\n",
    "\n",
    "<p class='lead'>\n",
    "    Here you will use a simple MCMC sampler to explore this 4-d parameter\n",
    "    space. We have already provided the sampler via ```smcmc.SimplerMCMC```,\n",
    "    but you can explore it more [here](https://github.com/vhaasteren/cit-busyweek/blob/master/day1/day1_solutions.ipynb).\n",
    "</p>\n",
    "\n",
    "<ol class='lead'>\n",
    "    <li> Draw initial search parameters from the prior using ```np.random.uniform```</li>\n",
    "    <li> Set jump ```sigmas``` to 0.05</li>\n",
    "    <li> Initialize sampler with loglike and logprior functions from your model object.</li>\n",
    "    <li> Run sampler for 100000 steps. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define initial parameters with np.random.uniform\n",
    "\n",
    "# define number of iterations and sigmas\n",
    "\n",
    "# setup sampler with smcmc.SimpleMCMC\n",
    "\n",
    "# sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Visualizing sampler output\n",
    "\n",
    "<p class='lead'>\n",
    "    Here you will perform some standard visualizations for analyzing MCMC output.\n",
    "    These plots are criticial to checking convergence of the MCMC and for visualizing\n",
    "    parameter covariances.   \n",
    "</p>\n",
    "\n",
    "<ol class='lead'>\n",
    "    <li> Set burn-in to be 25% of chain length</li>\n",
    "    <li> Plot the log posterior, ```sampler.lnprob``` vs iteration number. \n",
    "    This should be stable and not growing with iteration number. </li>\n",
    "    <li> Plot the parameter samples vs. iteration number for each of the 4 parameters. Again these should be stable and sampling around a centeral value.</li>\n",
    "    <li> Use the ```corner``` function to make a triangle plot of the 4 search parameters. Also, plot the injected parameters and the 1-sigma quantiles. See [here](https://github.com/dfm/corner.py) for help.</li>\n",
    "    <li> What can you say about the results of this run. Are the injections consistent with the posterior? What parameters are correlated?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set burn-in to be 25% of chain\n",
    "\n",
    "# plot log-posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make list of parameter labels\n",
    "pars = ['log-Amplitude', 'log-frequency', 'phase', 'log-sigma']\n",
    "\n",
    "# make list of injected parameters\n",
    "#inj = [np.log10(A), np.log10(freq), phase, np.log10(sigma)]\n",
    "\n",
    "# plot chain traces for 4 parameters\n",
    "#plt.figure(figsize=(10,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot triangle plot using corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Computing Bayesian Evidence via Nested Sampling\n",
    "\n",
    "<p class='lead'>\n",
    "    Here we will be computing the Bayes factor between the signal model\n",
    "    (i.e. signal + noise) and the noise model (i.e. signal=0 + noise). \n",
    "    Previously computing Bayes factor was easy because we simply could\n",
    "    integrate a 1-d function. Now, our posterior is 4-d and the integral\n",
    "    becomes much harder. There are several ways of computing the Bayesian \n",
    "    Evidence such as Nested Sampling, Reversible Jump MCMC, Thermodynamic \n",
    "    integration and others ([see this paper for a good review](http://arxiv.org/abs/0704.1808)).\n",
    "    <br><br>\n",
    "    Here we will be using a nested sampling implementation called [Nestle](http://kbarbary.github.io/nestle/).\n",
    "    We are using this mostly for simplicity as nested sampling is one of the easiest methods for\n",
    "    computing the Bayesian evidence in relatively small parameter spaces (i.e. < 50 parameters). \n",
    "    <br><br>\n",
    "    In the following section, we will modify the model class and compute the Bayes factor!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Modify Model class for Nestle and model selection\n",
    "\n",
    "<p class='lead'>\n",
    "    To run Nestle and to run our noise only model, we will have to\n",
    "    make some modifications to the Model class.\n",
    "    \n",
    "    <ol class=lead>\n",
    "        <li>Add extra option to initialize Model class with a specifice model. \n",
    "        full=signal+noise and null=noise only</li>\n",
    "        <li> Add an if statement to ```get_loglike``` to check for the model\n",
    "        and return the correct likelihood.</li>\n",
    "        <li>Define an extra function ```get_prior_transform``` to compute the prior\n",
    "        transform found [here](http://kbarbary.github.io/nestle/prior.html)</li>\n",
    "    </ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, x, t, pmin=[-9, -9, 0, -9], \n",
    "                 pmax=[-6, -7, 2*np.pi, -6], model='full'):\n",
    "        \"\"\"\n",
    "        Model class contains calls to log-likelihood and\n",
    "        log-prior functions. All calls to log-likelihood\n",
    "        and log-prior functions take in parameter array\n",
    "        in the order:\n",
    "        \n",
    "        0: log10 Amplitude [s]\n",
    "        1: log10 frequency [Hz]\n",
    "        2: phase [radian]\n",
    "        3: log10 sigma [s]\n",
    "        \n",
    "        if using full model and \n",
    "        \n",
    "        0: log10 sigma[s]\n",
    "        \n",
    "        if using null model\n",
    "        \n",
    "        :param x: Data vector\n",
    "        :param t: Times at which data is taken\n",
    "        :param pmin: Minimum values of model parameters\n",
    "        :param pmax: Maximum values of model parameters\n",
    "        :param model: Model type [full, null]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        self.t = t  \n",
    "        self.pmin = np.array(pmin)\n",
    "        self.pmax = np.array(pmax)\n",
    "        self.model = model\n",
    "    \n",
    "    def get_loglike(self, pars):\n",
    "        \n",
    "        # is self.model == 'full':\n",
    "            # do what you did before\n",
    "        # elif self.model == 'null':\n",
    "            # return noise only likelihood\n",
    "    \n",
    "        return loglike\n",
    "    \n",
    "    def get_logprior(self, pars):\n",
    "        \n",
    "        # same prior as above\n",
    "\n",
    "        return prior\n",
    "    \n",
    "    def get_prior_transform(self, cube):\n",
    "        \n",
    "        # return transformed prior as in the reference above\n",
    "        \n",
    "        return ptrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Run Nested sampling on signal + noise model\n",
    "\n",
    "<ol class='lead'>\n",
    "    <li> Initialize full model (i.e. just like before)</li>\n",
    "    <li> Run Nestle with ```nestle.sample``` using 300 live points and ```dlogz```=0.1</li>\n",
    "    <li> Print the summary with ```print result.summary()```</li>\n",
    "    <li> Inspect posterior with ```corner``` as above. However, this time you will\n",
    "    have to add a a few extra arguments such as ```weights=result.weights``` and \n",
    "    ```range=[0.997, 0.997, 0.997, 0.997]```. The weights are due to the fact that Nested\n",
    "    Sampling collects samples different than MCMC and the range is so that we zoom in on the \n",
    "    high probability regions.</li>\n",
    "    <li>Do these posteriors seem consistent with the MCMC?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize noise + signal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set ndim = 4\n",
    "# return nestle.sample result in fullresult\n",
    "# print fullresult.summar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call corner with modifications above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Run Nestle on null model\n",
    "\n",
    "<ol class='lead'>\n",
    "    <li> Initilize the null model. You will have to give the initialization\n",
    "    ```pmin=[-9], pmax=[-6], model='null'``` so that it knows to use the noise\n",
    "    only model.</li>\n",
    "    <li> Run Nestle with the same settings (remember to set ```ndim```=1 though)</li>\n",
    "    <li>Compute the log Bayes factor by computing the difference in logz between the two\n",
    "    models.</li>\n",
    "    <li>How significant is the result based on the Bayes factor interpretations mentioned in \n",
    "    problem 1.3.2?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize noise only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run nestle as above but set ndim = 1\n",
    "# return result in nullresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute Bayes factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Other injections\n",
    "\n",
    "<p class='lead'>\n",
    "    Repeat all of this section with injections with sinusoid amplitudes of \n",
    "    5, 15, and 30 ns. You can just change the amplitude above and re-evaluate the cells.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
